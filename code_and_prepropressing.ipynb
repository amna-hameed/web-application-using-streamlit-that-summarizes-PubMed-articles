{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d979513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "validate_df = pd.read_csv('validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa35586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             article  \\\n",
      "0  a recent systematic analysis showed that in 20...   \n",
      "1  it occurs in more than 50% of patients and may...   \n",
      "2  tardive dystonia ( td ) , a rarer side effect ...   \n",
      "3  lepidoptera include agricultural pests that , ...   \n",
      "4  syncope is caused by transient diffuse cerebra...   \n",
      "\n",
      "                                            abstract  \n",
      "0  background : the present study was carried out...  \n",
      "1  backgroundanemia in patients with cancer who a...  \n",
      "2  tardive dystonia ( td ) is a serious side effe...  \n",
      "3  many lepidopteran insects are agricultural pes...  \n",
      "4  we present an unusual case of recurrent cough ...  \n",
      "                                             article  \\\n",
      "0  anxiety affects quality of life in those livin...   \n",
      "1  small non - coding rnas are transcribed into m...   \n",
      "2  ohss is a serious complication of ovulation in...   \n",
      "3  congenital adrenal hyperplasia ( cah ) refers ...   \n",
      "4  type 1 diabetes ( t1d ) results from the destr...   \n",
      "\n",
      "                                            abstract  \n",
      "0  research on the implications of anxiety in par...  \n",
      "1  small non - coding rnas include sirna , mirna ...  \n",
      "2  objective : to evaluate the efficacy and safet...  \n",
      "3  congenital adrenal hyperplasia is a group of a...  \n",
      "4  objective(s):pentoxifylline is an immunomodula...  \n",
      "                                             article  \\\n",
      "0  venous thromboembolism ( vte ) comprising of d...   \n",
      "1  there is an epidemic of stroke in low and midd...   \n",
      "2  cardiovascular diseases account for the highes...   \n",
      "3  results of a liquid culturing system ( bd bact...   \n",
      "4  the need for magnetic resonance imaging ( mri ...   \n",
      "\n",
      "                                            abstract  \n",
      "0  background and aim : there is lack of substant...  \n",
      "1  backgroundthe questionnaire for verifying stro...  \n",
      "2  background : timely access to cardiovascular h...  \n",
      "3  to determine differences in the ability of myc...  \n",
      "4  aimsour aim was to evaluate the potential for ...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove unwanted characters, links, and unnecessary spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\[[^]]*\\]', '', text)  # Remove references in square brackets\n",
    "        text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the DataFrames\n",
    "train_df['article'] = train_df['article'].apply(preprocess_text)\n",
    "train_df['abstract'] = train_df['abstract'].apply(preprocess_text)\n",
    "\n",
    "test_df['article'] = test_df['article'].apply(preprocess_text)\n",
    "test_df['abstract'] = test_df['abstract'].apply(preprocess_text)\n",
    "\n",
    "validate_df['article'] = validate_df['article'].apply(preprocess_text)\n",
    "validate_df['abstract'] = validate_df['abstract'].apply(preprocess_text)\n",
    "\n",
    "# Check the preprocessed data\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(validate_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2cea5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained model and tokenizer from Hugging Face\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0002a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_data(df):\n",
    "    tokenized_inputs = []\n",
    "    tokenized_targets = []\n",
    "    \n",
    "    # Drop rows with NaN values in 'article' or 'abstract'\n",
    "    df = df.dropna(subset=['article', 'abstract'])\n",
    "    \n",
    "    for article, abstract in zip(df['article'], df['abstract']):\n",
    "        # Tokenize input text\n",
    "        input_ids = tokenizer.encode(\"summarize: \" + article, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "        tokenized_inputs.append(input_ids)\n",
    "    \n",
    "        # Tokenize target text (abstract)\n",
    "        target_ids = tokenizer.encode(abstract, max_length=150, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "        tokenized_targets.append(target_ids)\n",
    "    \n",
    "    dataset = {\n",
    "        'input_ids': torch.cat(tokenized_inputs, dim=0),\n",
    "        'labels': torch.cat(tokenized_targets, dim=0),\n",
    "    }\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Example usage with your DataFrame\n",
    "train_data = tokenize_data(train_df)\n",
    "validate_data = tokenize_data(validate_df)\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(article_text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + article_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f727b24",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: could not parse ModelProto from C:\\Users\\HP\\archive\\app.py",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15028\\162524209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load the fine-tuned model and tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\HP\\\\archive\\\\app.py\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\HP\\\\archive\\\\app.py\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2108\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2110\u001b[1;33m         return cls._from_pretrained(\n\u001b[0m\u001b[0;32m   2111\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2112\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[1;31m# Instantiate the tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2335\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2336\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2337\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2338\u001b[0m             raise OSError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madditional_special_tokens\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentencepiece\\__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentencepiece\\__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_EncodeAsIds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_bos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_eos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memit_unk_piece\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Internal: could not parse ModelProto from C:\\Users\\HP\\archive\\app.py"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"C:\\\\Users\\\\HP\\\\archive\\\\app.py\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"C:\\\\Users\\\\HP\\\\archive\\\\app.py\")\n",
    "\n",
    "def summarize_text(article_text):\n",
    "    inputs = tokenizer(\"summarize: \" + article_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, num_beams=2, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73712188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Collecting intel-openmp==2021.*\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Collecting tbb==2021.*\n",
      "  Using cached tbb-2021.13.0-py3-none-win_amd64.whl (286 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: tbb, intel-openmp, typing-extensions, mkl\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2aae48",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2400926375.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9416\\2400926375.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    conda create -n summarizer_env python=3.8\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_data(example):\n",
    "    input_text = \"summarize: \" + example['article']\n",
    "    target_text = example['abstract']\n",
    "    inputs = tokenizer(input_text, max_length=512, truncation=True, padding='max_length')\n",
    "    targets = tokenizer(target_text, max_length=150, truncation=True, padding='max_length')\n",
    "    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cda29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
